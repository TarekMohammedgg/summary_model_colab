{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hx0-mOaMQQpt"
   },
   "source": [
    "\n",
    "\n",
    "# Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pfHR-5ARwjjj"
   },
   "source": [
    "resouces : https://docs.cohere.com/v2/changelog/command-r7b-arabic\n",
    "\n",
    "translator Model resource : https://huggingface.co/Helsinki-NLP/opus-mt-ar-en/tree/main\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wgsldGmwWvFf"
   },
   "source": [
    "# convert notebook to ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iEnQA-sjCN2-"
   },
   "source": [
    "# pips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B3bbO3-tYxTU",
    "outputId": "0cb1d96c-56c5-408f-ac37-9dde669624a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/95.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/62.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m110.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m91.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m83.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q fastapi==0.115.12  nest-asyncio==1.6.0  pyngrok==7.2.5  uvicorn==0.34.2\n",
    "!pip install -q bitsandbytes==0.45.5\n",
    "!pip install -q accelerate==0.26.0 --progress-bar off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a-5Qz93ZObqZ"
   },
   "source": [
    "# Import Liberaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IZCO_rTwUkFN"
   },
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional, Literal\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM , BitsAndBytesConfig\n",
    "import torch\n",
    "import torch._dynamo\n",
    "import os\n",
    "from google.colab import drive\n",
    "from huggingface_hub import login\n",
    "import nest_asyncio\n",
    "from pyngrok import ngrok\n",
    "import uvicorn\n",
    "from google.colab import userdata\n",
    "from fastapi import FastAPI, HTTPException\n",
    "import re\n",
    "nest_asyncio.apply()\n",
    "hf_token  = \"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\"\n",
    "base_model_id = \"CohereForAI/c4ai-command-r7b-arabic-02-2025\"\n",
    "torch_dtype = torch.float16\n",
    "login(token=hf_token)\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "torch._dynamo.config.disable = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r7AsabCtMXMG"
   },
   "source": [
    "# Read Json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qLHlSYDcZYCR"
   },
   "outputs": [],
   "source": [
    "conversation_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "U9_4kBIKLJ-8",
    "outputId": "815df0bb-c5c1-406c-e068-b709e45bffb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient: ÙÙŠ Ø­ØªØ© ØºØ±ÙŠØ¨Ø© Ø¹Ù„Ù‰ Ø§Ù„ÙƒØªÙ ÙƒØ¯Ù‡ØŒ Ø¨Ù‚Øª ØªØªØºÙŠØ± Ù…Ø¹ Ø§Ù„ÙˆÙ‚Øª...\n",
      "\n",
      "Doctor: Ù…Ù† Ø¥Ù…ØªÙ‰ Ø¸Ù‡Ø±Øª Ø§Ù„Ø­ØªØ© Ø¯ÙŠØŸ\n",
      "\n",
      "Patient: ØªÙ‚Ø±ÙŠØ¨Ø§ Ù…Ù† 6 Ø´Ù‡ÙˆØ±...\n",
      "\n",
      "Doctor: Ù…Ù…ØªØ§Ø²ØŒ Ø£Ù†Ø§ Ø¹Ø§ÙŠØ²Ùƒ ØªØ§Ø®Ø¯ Ø¨Ø§Ù„Ùƒ...\n",
      "\n",
      "Patient: ÙÙŠ Ø§Ù„Ø£ÙˆÙ„ ÙƒØ§Ù†Øª Ø²ÙŠ Ø­Ø¨Ø© ÙˆØ±Ø¯ÙŠØ©...\n",
      "\n",
      "Doctor: Ù‡Ù„ Ù„Ø§Ø­Ø¸Øª Ø£ÙŠ Ø£Ø¹Ø±Ø§Ø¶ ØªØ§Ù†ÙŠØ© Ø²ÙŠ Ø­ÙƒØ©...\n",
      "\n",
      "Patient: Ø£ÙŠÙˆØ©ØŒ Ø¨ØªØ­Ùƒ Ø´ÙˆÙŠØ©...\n",
      "\n",
      "Doctor: Ù‡Ù„ Ø¬Ø±Ø¨Øª ØªØ³ØªØ®Ø¯Ù… Ø£ÙŠ Ù…Ø±Ù‡Ù…...\n",
      "\n",
      "Patient: Ù„Ø§.\n",
      "\n",
      "Doctor: Ù‡Ù„ ÙÙŠ Ø£ÙŠ Ø­Ø§Ø¬Ø© Ø¨ØªØ²ÙˆØ¯ Ø§Ù„Ù…Ø´ÙƒÙ„Ø©...\n",
      "\n",
      "Patient: Ù„Ø§.\n",
      "\n",
      "Doctor: Ù‡Ù„ Ø´ØºÙ„Ùƒ Ø¨ÙŠØªØ¹Ø±Ø¶ Ù„Ù„Ø´Ù…Ø³...\n",
      "\n",
      "Patient: Ø¢Ù‡ØŒ ÙƒÙ†Øª Ø´ØºØ§Ù„ ÙÙŠ Ø§Ù„Ø­Ù‚Ù„...\n",
      "\n",
      "Doctor: Ù‡Ù„ ÙÙŠ Ø£ÙŠ Ø­Ø¯ ÙÙŠ Ø¹ÙŠÙ„ØªÙƒ...\n",
      "\n",
      "Patient: Ø¬Ø¯ÙŠ ÙƒØ§Ù† Ø¹Ù†Ø¯Ù‡ Ù…ÙŠÙ„Ø§Ù†ÙˆÙ…Ø§...\n",
      "\n",
      "Doctor: Ù…Ù…ØªØ§Ø²ØŒ Ù…Ù…ÙƒÙ† Ù†Ø³ØªÙÙŠØ¯ Ù…Ù† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª...\n",
      "\n",
      "Patient: Ù„Ø§\n",
      "\n",
      "Doctor: Ù…Ù…ØªØ§Ø²ØŒ Ù‡Ø§Ø®Ø¯ Ø¨Ø§Ù„ÙŠ Ù…Ù† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª...\n"
     ]
    }
   ],
   "source": [
    "ls1 = {\n",
    "  \"response\": \"Ø´ÙƒØ±Ù‹Ø§ Ù„Ùƒ! Ø£ØªÙ…Ù†Ù‰ Ù„Ùƒ Ø§Ù„ØµØ­Ø© ÙˆØ§Ù„Ø¹Ø§ÙÙŠØ©. ğŸ˜Š\",\n",
    "  \"finished\": True,\n",
    "  \"full_conversation\": [\n",
    "    {\"role\": \"system\", \"content\": \"Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø·Ø¨ÙŠ Ø§ÙØªØ±Ø§Ø¶ÙŠ Ù…ØªØ®ØµØµ ...\"},\n",
    "    {\"role\": \"user\", \"content\": \"ÙÙŠ Ø­ØªØ© ØºØ±ÙŠØ¨Ø© Ø¹Ù„Ù‰ Ø§Ù„ÙƒØªÙ ÙƒØ¯Ù‡ØŒ Ø¨Ù‚Øª ØªØªØºÙŠØ± Ù…Ø¹ Ø§Ù„ÙˆÙ‚Øª...\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Ù…Ù† Ø¥Ù…ØªÙ‰ Ø¸Ù‡Ø±Øª Ø§Ù„Ø­ØªØ© Ø¯ÙŠØŸ\"},\n",
    "    {\"role\": \"user\", \"content\": \"ØªÙ‚Ø±ÙŠØ¨Ø§ Ù…Ù† 6 Ø´Ù‡ÙˆØ±...\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Ù…Ù…ØªØ§Ø²ØŒ Ø£Ù†Ø§ Ø¹Ø§ÙŠØ²Ùƒ ØªØ§Ø®Ø¯ Ø¨Ø§Ù„Ùƒ...\"},\n",
    "    {\"role\": \"user\", \"content\": \"ÙÙŠ Ø§Ù„Ø£ÙˆÙ„ ÙƒØ§Ù†Øª Ø²ÙŠ Ø­Ø¨Ø© ÙˆØ±Ø¯ÙŠØ©...\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Ù‡Ù„ Ù„Ø§Ø­Ø¸Øª Ø£ÙŠ Ø£Ø¹Ø±Ø§Ø¶ ØªØ§Ù†ÙŠØ© Ø²ÙŠ Ø­ÙƒØ©...\"},\n",
    "    {\"role\": \"user\", \"content\": \"Ø£ÙŠÙˆØ©ØŒ Ø¨ØªØ­Ùƒ Ø´ÙˆÙŠØ©...\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Ù‡Ù„ Ø¬Ø±Ø¨Øª ØªØ³ØªØ®Ø¯Ù… Ø£ÙŠ Ù…Ø±Ù‡Ù…...\"},\n",
    "    {\"role\": \"user\", \"content\": \"Ù„Ø§.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Ù‡Ù„ ÙÙŠ Ø£ÙŠ Ø­Ø§Ø¬Ø© Ø¨ØªØ²ÙˆØ¯ Ø§Ù„Ù…Ø´ÙƒÙ„Ø©...\"},\n",
    "    {\"role\": \"user\", \"content\": \"Ù„Ø§.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Ù‡Ù„ Ø´ØºÙ„Ùƒ Ø¨ÙŠØªØ¹Ø±Ø¶ Ù„Ù„Ø´Ù…Ø³...\"},\n",
    "    {\"role\": \"user\", \"content\": \"Ø¢Ù‡ØŒ ÙƒÙ†Øª Ø´ØºØ§Ù„ ÙÙŠ Ø§Ù„Ø­Ù‚Ù„...\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Ù‡Ù„ ÙÙŠ Ø£ÙŠ Ø­Ø¯ ÙÙŠ Ø¹ÙŠÙ„ØªÙƒ...\"},\n",
    "    {\"role\": \"user\", \"content\": \"Ø¬Ø¯ÙŠ ÙƒØ§Ù† Ø¹Ù†Ø¯Ù‡ Ù…ÙŠÙ„Ø§Ù†ÙˆÙ…Ø§...\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Ù…Ù…ØªØ§Ø²ØŒ Ù…Ù…ÙƒÙ† Ù†Ø³ØªÙÙŠØ¯ Ù…Ù† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª...\"},\n",
    "    {\"role\": \"user\", \"content\": \"Ù„Ø§\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Ù…Ù…ØªØ§Ø²ØŒ Ù‡Ø§Ø®Ø¯ Ø¨Ø§Ù„ÙŠ Ù…Ù† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª...\"}\n",
    "  ]\n",
    "}\n",
    "\n",
    "def convert_conversation_to_text(conversation):\n",
    "    result = []\n",
    "    for turn in conversation:\n",
    "        if turn[\"role\"] == \"assistant\":\n",
    "            result.append(f\"Doctor: {turn['content'].strip()}\")\n",
    "        elif turn[\"role\"] == \"user\":\n",
    "            result.append(f\"Patient: {turn['content'].strip()}\")\n",
    "    return \"\\n\\n\".join(result)\n",
    "\n",
    "conversation_text = convert_conversation_to_text(ls1[\"full_conversation\"])\n",
    "print(conversation_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xH1U8M4WLOfW"
   },
   "outputs": [],
   "source": [
    "conversation_list.append(conversation_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2-42ZWzpLSfq",
    "outputId": "2819f4dd-e1d4-4c19-d155-6291bf9f49a2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(conversation_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PENtJOh1CX9k"
   },
   "source": [
    "# Prepare Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MVVmB7H7O_j4"
   },
   "source": [
    "# Loading Cohere Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 457,
     "referenced_widgets": [
      "bd7d85643da043dca5b317e5b8ed86ef",
      "8ddad7db950c4a6c96ba50469cfb74d4",
      "cfdb28d72da64abca2519116023121b2",
      "22876b96c46f42c19dc0a2cff24cd3a9",
      "45725a286f8540a18af659c4ad078c6e",
      "9d4caf7416c8472ba1272555f85b6d39",
      "023545aff51943d6841c61ec5bf0c45c",
      "94c35cf81ac14716a26f3a783ec6e197",
      "16bec43d91464c3b9d9ac42e8c624f3c",
      "1a8dec025be84ec68b8a610ce794d6bc",
      "37bac3e3fd6948ee98c578b1cbe39095",
      "5566d4d1ecb94697a39d9b245b51cb1b",
      "9df7c0eebe7c4b1f88c7f2fc0843ced5",
      "d7ce25a01e6b460aaefbb30557301722",
      "f277eba6c0e6477a99b21c19df766204",
      "46d3da633e874c998895c8eda5266a4e",
      "a5b7a1d2d206431998bdf3f48eb977a1",
      "784e4feff70e4ae9a64438d42fb5a609",
      "a11599c14c1e4063b6395cd4976c268d",
      "1075b057798a4c89863e811107c388da",
      "da1f54594946458eb546a254fcf454bf",
      "6ad4b186d7d048669231b4b642548347",
      "6f94f871d58d43d1b667aa785484a66a",
      "45ce3afd6fdb4e10b853a751cbe89182",
      "66838fb5b004476d8366aa278f310ca2",
      "616c6ac8fbaa4718993457ca0bd874ca",
      "d93ad2fdc4544b1084090c910c92d8b7",
      "abd9299ce1a04887a12f9512887ae300",
      "56b9428fc602412394eaca9abe56de6f",
      "2adb05c7dae64f49b92f354d25339736",
      "8b05202ea76c406f872afad4517e2e16",
      "79348746b88944659f9b5fdb96594ed7",
      "3d7557bcecb5484d88b5774bb128af08",
      "810433d818d146c38b01cc319ddb88d6",
      "fb1703a065cc47cebdad81af59f51e59",
      "2ce21468edcc43988e44fe88e7dabefa",
      "50a6e5cf5e2646eebf13767514b373e8",
      "a78fc98c11484d5db73154c5be397ac8",
      "81c1693795524864991f3523985c81f6",
      "8a95e9f01d7445f9b43b72be8b7296c7",
      "bbf69910653d485db58015b7ef2b507f",
      "8eea7a8eb7324a8aafb4741bea7498d7",
      "ab82f2b876724d5a987e97aa774c5d2e",
      "f967cebcbde0499d8b2145b97ceaa9ce",
      "aaecc6a3ce8f453fb91c4248d891a66c",
      "7ae604de8cc542169f8c0198983d6e9e",
      "4ced07274f8d4d82bdd85630b59c0cbc",
      "c8c2764ed0b349ba8b9e5193d65a9beb",
      "247af01074544d64ab750dbaa7a47296",
      "3c0ee46a62d64d68821bdda547a850d3",
      "1480905f5f67494ebac35a02a14d408f",
      "6d0e375d627044c581cbfe2c88286750",
      "3d0b5fc84deb47ae8772110833598050",
      "cb006b0bf5af45a995dfa54125843650",
      "f83fd6c6a99c46c8b2517f853303d3d3",
      "1eee250ee2e94bc2a3debe71806cd830",
      "0082c451af9048b787c3040970e70f33",
      "77d3a07e798946e9abc79df9009c228b",
      "0acc880e44d3411bbc04a485ce49c1c7",
      "7da8ff34ea3b4131be5fa4e2d21b53a3",
      "766e6bc9434546d89627b8f8945e665d",
      "bc9541e30083499798f4a9a5ee4c31f8",
      "00c50f4b507c4ba2a831f23e6883fe15",
      "c4a9f53273d749e79cf8add0a81fa7ce",
      "1785501d7aa14dff93645b427618b580",
      "4775bcb2a64b4414b9b5338106ac2cd6",
      "0147ee5af7fc40779212f6e447559028",
      "840a3b64e0cc4dbf8f4e167f265d4bdf",
      "db83f35d976c410b91eba14ccc0104fb",
      "84ac93b268fa4ff0993e527ff6317f9c",
      "94704ad655cc4ac9a0ffc73aef59ca67",
      "3610d40a03934b8da8921705d5c7dfb0",
      "0bc9651d1d1e451da95a5b458bd77d0b",
      "55f8f860585d41faad1344a26c8490d2",
      "5447ed4ee31b410e8415014214dfea3a",
      "35374be3bec74342836f37115a4424a4",
      "a0526e599f2d4218b2706f7ff2fa4e25",
      "ba80b8fc17ff4297b1533ac206c6ab88",
      "10a83986de0e407ab254d08a6dc1e870",
      "7e22466bddc34bda807b64cad84144fc",
      "299a6c5377d542f488a39790ad476baf",
      "eac84eb4501b4139ba01b00c5a0b6eae",
      "9c668626c7c54688b16c8a11243f4214",
      "37ac7f9c6fcf43b99808bdec641aed45",
      "dcfcf4ef836d4d32912c0e94e004883e",
      "0212d0e22b0c4b55b96d23344e01ae6b",
      "45846320699d49cb86671d8154d1e37d",
      "8b158302c2934fbe9cbe5d87029594cc",
      "ce00de258d23449baed8b4f75957d2b0",
      "6552f89cf1aa42a5bfd869f02f4284c7",
      "d85bfb60449d4e79b487a99b812fd91e",
      "d8ee2b255f964446ab18baf0f5aafc9e",
      "641380d91fc14e0aabc0bf24a1ec14c1",
      "7b7ad0ce52e440a994be925e5b8d4e8a",
      "ad39ab7d50ed4a75bac9048e296bcb9d",
      "e99f03e4e2184eb9ba092c102335b51f",
      "6a508d9779a842039cd38a636eb32ab7",
      "5862c536db43467f84069767f2e482e0",
      "e68f0acd4f8c4e2b8189b355c09132be",
      "bb964fbcce2d4a6fbc238de02423b60f",
      "098041145def428f8216f3fd184844af",
      "15fca4a3c98d43c680718aca651bab47",
      "8cd9e1c562834d7aa343f73a3ae31ece",
      "ede3537e025643e3adb2a02d3f8198be",
      "9731e083e2624d5b8117615a892d4507",
      "14f4316a90814bc9a50b809498c00982",
      "0d08d99f616f4deea9d59045c70d2bf1",
      "fe73c9c77a3146e295e74049807699fd",
      "3e87b0e451524b0eb85f584094bdbcf4",
      "018366d9d4f74482bc4630bfa4917593",
      "39e395a6a472455199a3bbd666ffa839",
      "04931e7939f24e3bad6a67f3ee560ab6",
      "e22aaf58167f47788b7b34d8716a7c46",
      "5a5c79a868fe44c6bafd6f5f00f99534",
      "ae9d0aac53fb416194199ae0d160751b",
      "c9eef805d4814b03a328a79ea5082a35",
      "cb8c3be0992341c8a689b06138940f0c",
      "1b1d330463784175a8d4c4b3b7810fb7",
      "5e29883c41714fbd8aff9a9935af97eb",
      "e9e0178e2a4b405eb0824778efb5a9e5",
      "10b63e784c2a482db34526ad21f860f7",
      "d0ea3a70cec94a09875afba39c3317ad",
      "7bfd94659ef747a2b7c4b5952272d363",
      "123240aedf98437cab43795a98fd70d0",
      "01c484aa98974d2bad1bdd7e7cb7f461",
      "e259d046c0c842c996e651de399992dc",
      "5a38f15a434746efbea59a17a911eddb",
      "132ba80ca46b42beb4c0b135c914d9d8",
      "0f5f220a960944bf813cbdba72c120aa",
      "50cd68380d5b4990be538d4c057099f8",
      "9ce2320bc4584810a2812e388ccd740a",
      "be9225b6958446cb965703b4daabecec"
     ]
    },
    "id": "H80mbrFhUUQo",
    "outputId": "31d4c9ed-3cc0-4693-f991-2b5e5606f350"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd7d85643da043dca5b317e5b8ed86ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/999 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5566d4d1ecb94697a39d9b245b51cb1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/21.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f94f871d58d43d1b667aa785484a66a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "810433d818d146c38b01cc319ddb88d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaecc6a3ce8f453fb91c4248d891a66c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eee250ee2e94bc2a3debe71806cd830",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0147ee5af7fc40779212f6e447559028",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba80b8fc17ff4297b1533ac206c6ab88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce00de258d23449baed8b4f75957d2b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/178 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py:898: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb964fbcce2d4a6fbc238de02423b60f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/45.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39e395a6a472455199a3bbd666ffa839",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/20.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0ea3a70cec94a09875afba39c3317ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/664 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "# from peft import prepare_model_for_kbit_training\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "def load_quantized_model(model_name: str, load_in_4bit: bool = True,\n",
    "                         use_double_quant: bool = True,\n",
    "                         quant_type: str = \"nf4\",\n",
    "                         compute_dtype=torch.bfloat16,\n",
    "                         auth_token: bool = True):\n",
    "\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=load_in_4bit,\n",
    "        bnb_4bit_use_double_quant=use_double_quant,\n",
    "        bnb_4bit_quant_type=quant_type,\n",
    "        bnb_4bit_compute_dtype=compute_dtype,\n",
    "    )\n",
    "\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    max_memory = {i: '40960MB' for i in range(n_gpus)}\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        max_memory=max_memory,\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=auth_token)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "# Run\n",
    "model_name = base_model_id\n",
    "model, tokenizer = load_quantized_model(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ss0XFi1cY4KY",
    "outputId": "541f6c62-5a7f-4eff-d3f1-c24b539a9cb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(model.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zMOnxPq2POco"
   },
   "source": [
    "# Clear Cache GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GXSOIhbALUHU",
    "outputId": "b03d37bb-06ce-4e32-8781-9a09a30ea08f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU cache cleared.\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print(\"GPU cache cleared.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TYuuGlavPua9"
   },
   "source": [
    "# Translated Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b27FNM9YKp-S"
   },
   "outputs": [],
   "source": [
    "class TranslateModel(BaseModel):\n",
    "\n",
    "    Translated_conversation: str= Field(... , min_length = 5 ,description = \"The conversation translated from Arabic into English\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VHjIt5AyvK8b"
   },
   "outputs": [],
   "source": [
    "ar_text = conversation_list[0]\n",
    "\n",
    "def TranslateTemplate(ar_text: str, translate_model) :\n",
    "\n",
    "    return [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\\n\".join([\n",
    "              \"You are a professional translator.\",\n",
    "              \"You will be provided with an Arabic text.\",\n",
    "              \"Translate the text into English.\",\n",
    "              \"Extract JSON details according to the Pydantic schema provided.\",\n",
    "              \"Make sure that the values in the JSON are written in **English**.\",\n",
    "              \"Respond ONLY with a valid JSON object that follows the schema â€“ no markdown fences, no extra text.\"\n",
    "\n",
    "            ])\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"\\n\".join([\n",
    "                \"## conversation\",\n",
    "                ar_text,\n",
    "                \"\",\n",
    "                \"## Pydantic Details\",\n",
    "                json.dumps(translate_model.model_json_schema(), ensure_ascii=False),\n",
    "                \"\",\n",
    "                \"## Translation : \",\n",
    "                \"```json\"\n",
    "            ])\n",
    "        }\n",
    "    ]\n",
    "\n",
    "translate_template_message = TranslateTemplate(ar_text, TranslateModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "hqKTGa0Byalc",
    "outputId": "e0372c82-5390-4e1d-bcd2-922bdb7e35b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Translated_conversation\": \"In a strange place on the shoulder, it's been changing over time...\\n\\nDoctor: Since when has this place appeared?\\n\\nPatient: About 6 months ago...\\n\\nDoctor: Excellent, I want you to be careful...\\n\\nPatient: At first it was like a pinkish spot...\\n\\nDoctor: Have you noticed any other symptoms like itching...\\n\\nPatient: Yes, it itches a little...\\n\\nDoctor: Have you tried using any ointment...\\n\\nPatient: No.\\n\\nDoctor: Is there anything that makes the problem worse...\\n\\nPatient: No.\\n\\nDoctor: Is your job exposed to the sun...\\n\\nPatient: Yes, I worked in the field...\\n\\nDoctor: Is there anyone in your family...\\n\\nPatient: My grandfather had melanoma...\\n\\nDoctor: Excellent, we can use the information...\\n\\nPatient: No\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def TranslateGenerate(message, tokenizer, model, max_new_tokens=1500):\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        message,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(model.device)\n",
    "\n",
    "    gen_tokens = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=False,\n",
    "        temperature=None,\n",
    "    )\n",
    "\n",
    "    gen_tokens = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(input_ids, gen_tokens)\n",
    "    ]\n",
    "\n",
    "    gen_text = tokenizer.decode(gen_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "    match = re.search(r'\\{.*?\\}', gen_text, re.DOTALL)\n",
    "    if not match:\n",
    "        raise ValueError(\"No JSON block found in model output\")\n",
    "\n",
    "    json_block = match.group(0).strip()\n",
    "    return json.loads(json_block)\n",
    "\n",
    "# Example Usage:\n",
    "translated_json = TranslateGenerate(translate_template_message, tokenizer, model)\n",
    "translate = json.dumps(translated_json, ensure_ascii=False, indent=2)\n",
    "print(translate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CFXhe7zVPyQl"
   },
   "source": [
    "# Summarization Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FyG9o7VJSo0Q"
   },
   "outputs": [],
   "source": [
    "class SummaryModel(BaseModel):\n",
    "\n",
    "    Patient_symptoms: List[str] = Field(... , min_length = 5 , max_length = 300 ,description = \"Key symptoms mentioned\" )\n",
    "    Symptom_location: str = Field(... , min_length = 5 , max_length = 300 ,description = \"Affected area\" )\n",
    "    Duration: str = Field(... , min_length = 5 , max_length = 300 ,description = \"How long the symptoms have persisted\" )\n",
    "    Symptom_progression: str = Field(... , min_length = 5 , max_length = 300 ,description = \"How they have changed over time\" )\n",
    "    Risk_factors: str = Field(... , min_length = 5 , max_length = 300 ,description = \"Sun exposure, family history, etc.\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jH3cO8auSzqC"
   },
   "outputs": [],
   "source": [
    "en_text = translated_json['Translated_conversation']\n",
    "\n",
    "def SummaryTemplate(en_text: str, summary_model) :\n",
    "\n",
    "    return [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\\n\".join([\n",
    "                \"You are an NLP data paraser.\",\n",
    "                \"You will be provided by an Arabic text associated with a Pydantic scheme.\",\n",
    "                \"Generate the ouptut as same as  language.\",\n",
    "                \"You have to extract JSON details from text according the Pydantic details.\",\n",
    "                \"Extract details as mentioned in text.\",\n",
    "                \"Do not generate any introduction or conclusion.\"\n",
    "            ])\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"\\n\".join([\n",
    "                \"## conversation\",\n",
    "                en_text,\n",
    "                \"\",\n",
    "                \"## Pydantic Details\",\n",
    "                json.dumps(summary_model.model_json_schema(), ensure_ascii=False),\n",
    "                \"\",\n",
    "                \"## Summarization : \",\n",
    "                \"```json\"\n",
    "            ])\n",
    "        }\n",
    "    ]\n",
    "summary_template_message = SummaryTemplate(en_text, SummaryModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-D-l-xOcwZ5X"
   },
   "outputs": [],
   "source": [
    "\n",
    "def SummaryGenerate(message, tokenizer, model, max_new_tokens=200):\n",
    "    # Tokenize the input message\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        message,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(model.device)\n",
    "\n",
    "    # Generate response tokens\n",
    "    gen_tokens = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=False,\n",
    "        temperature=None,\n",
    "    )\n",
    "\n",
    "    # Extract the generated tokens after the input prompt\n",
    "    gen_tokens = [\n",
    "        output_ids[len(input_ids):]\n",
    "        for input_ids, output_ids in zip(input_ids, gen_tokens)\n",
    "    ]\n",
    "\n",
    "    # Decode and clean the generated text\n",
    "    gen_text = tokenizer.decode(gen_tokens[0])\n",
    "    gen_text = gen_text.replace(\"<|END_RESPONSE|>\", \"\").replace(\"<|END_OF_TURN_TOKEN|>\", \"\").replace(\"```\" , \"\").strip()\n",
    "    match = re.search(r'\\{.*?\\}', gen_text, re.DOTALL)\n",
    "    if not match:\n",
    "        raise ValueError(\"No JSON block found in model output\")\n",
    "\n",
    "    json_block = match.group(0).strip()\n",
    "\n",
    "    return json.loads(json_block)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qoqSImhaR-D_"
   },
   "source": [
    "# User Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rnuk8PCuR8kn",
    "outputId": "d7bb04ed-e5b7-46de-cdfd-ed5480424c98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Patient_symptoms': ['Itching'], 'Symptom_location': 'On the shoulder', 'Duration': 'About 6 months', 'Symptom_progression': 'It started as a pinkish spot and has been changing over time', 'Risk_factors': 'Exposure to the sun, family history of melanoma'}\n"
     ]
    }
   ],
   "source": [
    "# Example Usage:\n",
    "generated_text = SummaryGenerate(summary_template_message, tokenizer, model)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ty0riGW1SeAO",
    "outputId": "b256ec07-594f-4c57-e7ca-94a9e8e09783"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Patient_symptoms': ['Itching'],\n",
       " 'Symptom_location': 'On the shoulder',\n",
       " 'Duration': 'About 6 months',\n",
       " 'Symptom_progression': 'It started as a pinkish spot and has been changing over time',\n",
       " 'Risk_factors': 'Exposure to the sun, family history of melanoma'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gh1M9XGNfyVf"
   },
   "source": [
    "#Create ApI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D1Cz7Jfcb7C3"
   },
   "outputs": [],
   "source": [
    "app = FastAPI()\n",
    "\n",
    "# Define conversation turn model\n",
    "class Turn(BaseModel):\n",
    "    role: str\n",
    "    content: str\n",
    "\n",
    "# Define input model with list directly\n",
    "class SummarizationInput(BaseModel):\n",
    "    messages: List[Turn]\n",
    "def convert_conversation_to_text_for_api(conversation: List[Turn]) -> str:\n",
    "    result = []\n",
    "    for turn in conversation:\n",
    "        if turn.role == \"assistant\":\n",
    "            result.append(f\"Doctor: {turn.content.strip()}\")\n",
    "        elif turn.role == \"user\":\n",
    "            result.append(f\"Patient: {turn.content.strip()}\")\n",
    "    return \"\\n\\n\".join(result)\n",
    "\n",
    "@app.post(\"/summarize\")\n",
    "def summarize(input_data: SummarizationInput):\n",
    "    try:\n",
    "        # Convert structured list to readable conversation text\n",
    "        conversation_text = convert_conversation_to_text_for_api(input_data.messages)\n",
    "\n",
    "        # Translate\n",
    "        translate_template_message = TranslateTemplate(conversation_text, TranslateModel)\n",
    "        translate_generate_message = TranslateGenerate(translate_template_message, tokenizer, model)\n",
    "        en_text = translate_generate_message['Translated_conversation']\n",
    "\n",
    "        # Summarize\n",
    "        summarize_template_message = SummaryTemplate(en_text, SummaryModel)\n",
    "        summary_generate_message = SummaryGenerate(summarize_template_message, tokenizer, model)\n",
    "\n",
    "        return summary_generate_message\n",
    "\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pEImw0zzvu9h"
   },
   "outputs": [],
   "source": [
    "## Test API Function\n",
    "# test = summarize(SummarizationInput(messages=ls1[\"full_conversation\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zyiVz3i80aV9",
    "outputId": "77878d3f-ecb4-463c-ff31-96d44a361d6e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Patient_symptoms': ['Itching'],\n",
       " 'Symptom_location': 'On the shoulder',\n",
       " 'Duration': 'About 6 months',\n",
       " 'Symptom_progression': 'It started as a pinkish spot and has been changing over time',\n",
       " 'Risk_factors': 'Exposure to the sun, family history of melanoma'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "io4PCRYz1ANl",
    "outputId": "c090cfb1-e495-440b-e7ac-893ab9a8feab"
   },
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Your API is live at: NgrokTunnel: \"https://034c-34-142-252-49.ngrok-free.app\" -> \"http://localhost:8001\"\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [428]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8001 (Press CTRL+C to quit)\n",
      "WARNING:pyngrok.process.ngrok:t=2025-05-16T11:51:04+0000 lvl=warn msg=\"failed to check for update\" obj=updater err=\"Post \\\"https://update.equinox.io/check\\\": context deadline exceeded\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     156.222.7.70:0 - \"GET / HTTP/1.1\" 404 Not Found\n",
      "INFO:     156.222.7.70:0 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\n",
      "INFO:     156.222.7.70:0 - \"GET /docs HTTP/1.1\" 200 OK\n",
      "INFO:     156.222.7.70:0 - \"GET /openapi.json HTTP/1.1\" 200 OK\n",
      "INFO:     156.222.7.70:0 - \"POST /summarize HTTP/1.1\" 200 OK\n",
      "INFO:     156.222.7.70:0 - \"GET / HTTP/1.1\" 404 Not Found\n",
      "INFO:     84.36.63.45:0 - \"GET / HTTP/1.1\" 404 Not Found\n",
      "INFO:     84.36.63.45:0 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\n",
      "INFO:     84.36.63.45:0 - \"GET /docs HTTP/1.1\" 200 OK\n",
      "INFO:     84.36.63.45:0 - \"GET /openapi.json HTTP/1.1\" 200 OK\n",
      "INFO:     84.36.63.45:0 - \"POST /summarize HTTP/1.1\" 200 OK\n",
      "INFO:     84.36.63.45:0 - \"POST /summarize HTTP/1.1\" 200 OK\n",
      "INFO:     84.36.63.45:0 - \"POST /summarize HTTP/1.1\" 200 OK\n",
      "INFO:     156.222.7.70:0 - \"GET / HTTP/1.1\" 404 Not Found\n",
      "INFO:     156.222.7.70:0 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\n",
      "INFO:     156.222.7.70:0 - \"GET /docs HTTP/1.1\" 200 OK\n",
      "INFO:     156.222.7.70:0 - \"GET /openapi.json HTTP/1.1\" 200 OK\n",
      "INFO:     156.222.7.70:0 - \"POST /summarize HTTP/1.1\" 200 OK\n",
      "INFO:     84.36.63.45:0 - \"POST /summarize HTTP/1.1\" 200 OK\n",
      "INFO:     84.36.63.45:0 - \"POST /summarize HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "nest_asyncio.apply()\n",
    "NGROK_AUTH_TOKEN = userdata.get(\"NGROK_AUTH_TOKEN\")\n",
    "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
    "for tunnel in ngrok.get_tunnels():\n",
    "    ngrok.disconnect(tunnel.public_url)\n",
    "public_url = ngrok.connect(8001)\n",
    "print(\"ğŸš€ Your API is live at:\", public_url)\n",
    "uvicorn.run(app, host=\"0.0.0.0\", port=8001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D0yKa_f248Sf"
   },
   "outputs": [],
   "source": [
    "!lsof -i:8001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B07nREEC4_fw"
   },
   "outputs": [],
   "source": [
    "!kill -9 <PID>\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "zMOnxPq2POco"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
